"""
–£—Ç–∏–ª–∏—Ç—ã –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
"""

import os
import json
import pickle
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union
from pathlib import Path
from config import DIRS

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataValidator:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""

    @staticmethod
    def validate_crypto_data(data: pd.DataFrame, crypto_name: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã"""
        required_columns = ["Open", "High", "Low", "Close", "Volume"]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–æ–∫
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è {crypto_name}: {missing_columns}")
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
        if data.empty:
            logger.error(f"–ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {crypto_name}")
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
        price_columns = ["Open", "High", "Low", "Close"]
        for col in price_columns:
            if (data[col] <= 0).any():
                logger.warning(
                    f"–ù–∞–π–¥–µ–Ω—ã –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∏–ª–∏ –Ω—É–ª–µ–≤—ã–µ —Ü–µ–Ω—ã –≤ {col} –¥–ª—è {crypto_name}"
                )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏–∫–∏ High >= Low
        if (data["High"] < data["Low"]).any():
            logger.error(f"–ù–∞—Ä—É—à–µ–Ω–∞ –ª–æ–≥–∏–∫–∞ High >= Low –¥–ª—è {crypto_name}")
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—ã–±—Ä–æ—Å—ã (—Ü–µ–Ω—ã, –æ—Ç–ª–∏—á–∞—é—â–∏–µ—Å—è –±–æ–ª–µ–µ —á–µ–º –≤ 10 —Ä–∞–∑)
        price_ratios = data["Close"].pct_change().abs()
        outliers = price_ratios > 5.0  # 500% –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞ –¥–µ–Ω—å
        if outliers.any():
            logger.warning(
                f"–ù–∞–π–¥–µ–Ω—ã –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã –≤ –¥–∞–Ω–Ω—ã—Ö {crypto_name}: "
                f"{outliers.sum()} —Ç–æ—á–µ–∫"
            )

        return True

    @staticmethod
    def validate_features(features_df: pd.DataFrame) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
        nan_count = features_df.isnull().sum().sum()
        if nan_count > 0:
            logger.warning(f"–ù–∞–π–¥–µ–Ω–æ {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        inf_count = np.isinf(features_df.select_dtypes(include=[np.number])).sum().sum()
        if inf_count > 0:
            logger.error(f"–ù–∞–π–¥–µ–Ω–æ {inf_count} –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        if len(features_df) < 100:
            logger.warning(f"–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(features_df)} –∑–∞–ø–∏—Å–µ–π")

        return True


class ModelUtils:
    """–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏"""

    @staticmethod
    def save_model_info(
        model, crypto_name: str, model_type: str, metrics: Dict, parameters: Dict
    ):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        model_info = {
            "crypto_name": crypto_name,
            "model_type": model_type,
            "metrics": metrics,
            "parameters": parameters,
            "created_at": datetime.now().isoformat(),
            "model_summary": ModelUtils._get_model_summary(model),
        }

        info_file = DIRS["models"] / f"{crypto_name}_{model_type}_info.json"
        with open(info_file, "w") as f:
            json.dump(model_info, f, indent=4, default=str)

        logger.info(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {info_file}")

    @staticmethod
    def _get_model_summary(model) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –æ –º–æ–¥–µ–ª–∏"""
        return {
            "total_params": model.count_params(),
            "trainable_params": int(
                np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])
            ),
            "layers_count": len(model.layers),
            "input_shape": str(model.input_shape),
            "output_shape": str(model.output_shape),
        }

    @staticmethod
    def load_model_info(crypto_name: str, model_type: str) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        info_file = DIRS["models"] / f"{crypto_name}_{model_type}_info.json"

        if info_file.exists():
            with open(info_file, "r") as f:
                return json.load(f)
        return None

    @staticmethod
    def calculate_model_confidence(predictions: np.ndarray, actual: np.ndarray) -> Dict:
        """–†–∞—Å—á–µ—Ç –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤"""
        errors = predictions - actual

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
        error_std = np.std(errors)

        # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã (95%)
        confidence_95 = 1.96 * error_std

        # –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ
        within_confidence = np.abs(errors) <= confidence_95
        confidence_accuracy = np.mean(within_confidence) * 100

        return {
            "error_std": float(error_std),
            "confidence_95": float(confidence_95),
            "confidence_accuracy": float(confidence_accuracy),
        }


class FileManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∞"""

    @staticmethod
    def cleanup_old_files(days_old: int = 30):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        cutoff_date = datetime.now() - timedelta(days=days_old)

        for directory in DIRS.values():
            if not directory.exists():
                continue

            for file_path in directory.iterdir():
                if file_path.is_file():
                    file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if file_time < cutoff_date:
                        try:
                            file_path.unlink()
                            logger.info(f"–£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {file_path}")
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {file_path}: {e}")

    @staticmethod
    def get_project_size() -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ –ø–∞–ø–∫–∞–º"""
        sizes = {}

        for name, directory in DIRS.items():
            if directory.exists():
                total_size = sum(
                    f.stat().st_size for f in directory.rglob("*") if f.is_file()
                )
                sizes[name] = {
                    "bytes": total_size,
                    "mb": round(total_size / (1024 * 1024), 2),
                    "files_count": len(list(directory.rglob("*"))),
                }
            else:
                sizes[name] = {"bytes": 0, "mb": 0, "files_count": 0}

        return sizes

    @staticmethod
    def export_results_to_excel(session_results: Dict, filename: str = None):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Excel"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crypto_analysis_results_{timestamp}.xlsx"

        file_path = DIRS["metrics"] / filename

        with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
            # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫
            metrics_data = []
            for crypto_name, results in session_results.items():
                for model_key, metrics in results.get("metrics", {}).items():
                    model_type = model_key.split("_")[-1]
                    metrics_data.append(
                        {"–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": crypto_name, "–ú–æ–¥–µ–ª—å": model_type, **metrics}
                    )

            if metrics_data:
                metrics_df = pd.DataFrame(metrics_data)
                metrics_df.to_excel(writer, sheet_name="–ú–µ—Ç—Ä–∏–∫–∏", index=False)

            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–∞–∂–¥–æ–π –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–µ
            for crypto_name, results in session_results.items():
                if "features_df" in results:
                    features_df = results["features_df"]
                    sheet_name = crypto_name[
                        :31
                    ]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Excel –Ω–∞ –¥–ª–∏–Ω—É –∏–º–µ–Ω–∏ –ª–∏—Å—Ç–∞
                    features_df.to_excel(writer, sheet_name=sheet_name)

        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ Excel: {file_path}")
        return file_path


class MetricsCalculator:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π"""

    @staticmethod
    def calculate_advanced_metrics(
        y_true: np.ndarray, y_pred: np.ndarray, prices: np.ndarray = None
    ) -> Dict:
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

        metrics = {}

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics["MSE"] = float(mean_squared_error(y_true, y_pred))
        metrics["RMSE"] = float(np.sqrt(metrics["MSE"]))
        metrics["MAE"] = float(mean_absolute_error(y_true, y_pred))
        metrics["R2"] = float(r2_score(y_true, y_pred))

        # MAPE
        with np.errstate(divide="ignore", invalid="ignore"):
            mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
            metrics["MAPE"] = float(mape) if np.isfinite(mape) else float("inf")

        # Symmetric MAPE
        smape = (
            np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))
            * 100
        )
        metrics["SMAPE"] = float(smape)

        # Directional Accuracy
        if len(y_true) > 1:
            actual_direction = np.diff(y_true) > 0
            pred_direction = np.diff(y_pred) > 0
            metrics["Directional_Accuracy"] = float(
                np.mean(actual_direction == pred_direction) * 100
            )

        # Theil's U —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if len(y_true) > 1:
            theil_u = MetricsCalculator._calculate_theil_u(y_true, y_pred)
            metrics["Theil_U"] = float(theil_u)

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞
        metrics["Max_Error"] = float(np.max(np.abs(y_true - y_pred)))

        # –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        error_5pct = np.abs((y_true - y_pred) / y_true) <= 0.05
        error_10pct = np.abs((y_true - y_pred) / y_true) <= 0.10

        metrics["Accuracy_5pct"] = float(np.mean(error_5pct) * 100)
        metrics["Accuracy_10pct"] = float(np.mean(error_10pct) * 100)

        return metrics

    @staticmethod
    def _calculate_theil_u(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç Theil's U —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        actual_changes = np.diff(y_true)
        pred_changes = np.diff(y_pred)

        numerator = np.sqrt(np.mean((actual_changes - pred_changes) ** 2))
        denominator = np.sqrt(np.mean(actual_changes**2)) + np.sqrt(
            np.mean(pred_changes**2)
        )

        return numerator / denominator if denominator != 0 else 0


class ConfigManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""

    @staticmethod
    def save_user_preferences(preferences: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        config_file = DIRS["metrics"] / "user_preferences.json"

        with open(config_file, "w") as f:
            json.dump(preferences, f, indent=4)

        logger.info("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

    @staticmethod
    def load_user_preferences() -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        config_file = DIRS["metrics"] / "user_preferences.json"

        if config_file.exists():
            with open(config_file, "r") as f:
                return json.load(f)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return {
            "default_period": "2y",
            "default_epochs": 50,
            "default_models": ["LSTM"],
            "use_macro": True,
            "auto_save_plots": True,
            "verbose": False,
        }


class Logger:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""

    @staticmethod
    def setup_logging(log_level: str = "INFO", log_file: str = None):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

        if log_file is None:
            log_file = (
                DIRS["logs"]
                / f"crypto_predictor_{datetime.now().strftime('%Y%m%d')}.log"
            )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∞–π–ª–æ–≤–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
        file_handler = logging.FileHandler(log_file, encoding="utf-8")
        file_handler.setFormatter(logging.Formatter(log_format))

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Å–æ–ª—å–Ω–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(logging.Formatter(log_format))

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ—Ä–Ω–µ–≤–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
        root_logger = logging.getLogger()
        root_logger.setLevel(getattr(logging, log_level.upper()))
        root_logger.addHandler(file_handler)
        root_logger.addHandler(console_handler)

        logger.info(f"–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ. –§–∞–π–ª: {log_file}")


def create_project_structure():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""
    print("üìÅ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞...")

    for name, directory in DIRS.items():
        if not directory.exists():
            directory.mkdir(exist_ok=True, parents=True)
            print(f"‚úì –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: {directory}")
        else:
            print(f"  –ü–∞–ø–∫–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {directory}")

    # –°–æ–∑–¥–∞–Ω–∏–µ requirements.txt
    requirements_path = Path("requirements.txt")
    if not requirements_path.exists():
        requirements = [
            "numpy>=1.21.0",
            "pandas>=1.3.0",
            "matplotlib>=3.4.0",
            "seaborn>=0.11.0",
            "scikit-learn>=0.24.0",
            "tensorflow>=2.6.0",
            "yfinance>=0.1.70",
            "plotly>=5.0.0",
            "openpyxl>=3.0.0",
            "joblib>=1.0.0",
        ]

        with open(requirements_path, "w") as f:
            f.write("\n".join(requirements))

        print(f"‚úì –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {requirements_path}")

    print("\n‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")


def format_price(price: float, currency: str = "$") -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω—ã"""
    return f"{currency}{price:,.2f}"


def format_percentage(value: float) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤"""
    return f"{value:+.2f}%"


def calculate_profit_loss(
    initial_price: float, final_price: float, amount: float = 1.0
) -> Dict:
    """–†–∞—Å—á–µ—Ç –ø—Ä–∏–±—ã–ª–∏/—É–±—ã—Ç–∫–∞"""
    profit_loss = (final_price - initial_price) * amount
    profit_loss_pct = ((final_price - initial_price) / initial_price) * 100

    return {
        "initial_value": initial_price * amount,
        "final_value": final_price * amount,
        "profit_loss": profit_loss,
        "profit_loss_pct": profit_loss_pct,
        "is_profit": profit_loss > 0,
    }


def generate_report_summary(session_results: Dict) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    report = []
    report.append("=" * 60)
    report.append("üìä –û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –ö–†–ò–ü–¢–û–í–ê–õ–Æ–¢")
    report.append("=" * 60)
    report.append(f"–î–∞—Ç–∞: {datetime.now().strftime('%d.%m.%Y %H:%M')}")
    report.append("")

    for crypto_name, results in session_results.items():
        report.append(f"\nü™ô {crypto_name}")
        report.append("-" * 40)

        # –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π
        for model_key, metrics in results.get("metrics", {}).items():
            model_type = model_key.split("_")[-1]
            report.append(f"\n–ú–æ–¥–µ–ª—å: {model_type}")
            report.append(f"  RMSE: ${metrics['RMSE']:.2f}")
            report.append(f"  MAE: ${metrics['MAE']:.2f}")
            report.append(f"  R¬≤: {metrics['R2']:.4f}")
            report.append(f"  MAPE: {metrics['MAPE']:.2f}%")
            report.append(
                f"  –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {metrics['Directional_Accuracy']:.1f}%"
            )

    report.append("\n" + "=" * 60)

    return "\n".join(report)


def save_report(report_text: str, filename: str = None):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"crypto_report_{timestamp}.txt"

    report_path = DIRS["metrics"] / filename

    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_text)

    logger.info(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
    return report_path
